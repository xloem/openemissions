- [ ] find local testing facility and see how much they would charge to determine SE of homemade junk
  -> sent e-mail to jansky lab 2018-10-19
- [ ] new detector approach will make use of accumulated floats with a divisor
    may need to switch to rational numbers to fix this
    - [X] debug mean difference issue and verify it will be fixed by a switch to rational numbers
        -> issue was nonpresent when debugging.  magnitude of number does not rise high enough.
    - X[ ] switch to arbitrary-precision numbers
    - X[ ] verify that this makes the total mean equal to the accumulated mean
- [ ] add TODO to adjust for signal ppm.  due to a bug i had thought it wasn't an issue
    should test multiple runs of device to understand when it changes
    -> I'm thinking, for rtl-sdr, is useful to have clock-time and sample-time separate for buffers.  can infer ppm from clock time, but note accuracy is low because inclueds software latency
       could also just note that each session will haev a different ppm
       -> probably take known freq of source and infer combined ppm?
       -> to infer ppm in future, will want id of radio stored with recording
- [X] find where I wrote belief that ppm was not an issue, and update
    -> one spot, perhaps only one, was in README.txt in this dir
- [ ] provide for detector to _stop_ when it detects signal, and update phase.
     will need to store multiple statistical models as signal chunks
     each chunk has a count and a time
     chunks can be combined to determine phase
  -> this behavior could replace current profiler
  - [ ] some notes and checkboxes on this at bottom of file.  it's an exciting approach but involves accumulation of scalars

1. ::: PeriodFinderConvolveDownsample :::
  We want to profile the signal when it is loud.
  Given a profile, we can determine the weakest we can detect given our detection approach.
  1. downsample to H * 2
  2. Convolve each possible period to map onsets
      Use gaussian stats bins and mark a TODO to reference actual underlying data for more precise stats
  3. Decrease downsampling and loop to 2 to get a map for raw buffer size
      Mark a TODO to store detailed stats information on onsets to report precise situation
      Mark a TODO to consider adjusting downsampled bins to properly wrap upsampled bins
  4. update data storage

      -> before settling on this, let's measure multiple runs to determine if frequency stays constant

      3 separate files stored such that they may be written to live
      - wavefile -> stores information on signal
          - avg period
          - max precision of avg period
          - std deviation of period if greater than sample time present
          - add TODO: store model wave
              - add TODO: consider array of underlying frequencies and store detailed distributions for each possible frequency component, adjusting properly for phase drift
                DON'T do this now it will slow you down
          - refs to recfiles
      - recfile -> stores information on a recording of signal
          - ref to logfile
          - list of onset times
      - logfile
          - raw recording of samples in standard format
          - information like frequency tuning and recording timing would go here, NOT in the recfile
            this is to keep the recfile specific to the profiling behavior; this is a standard format
            these are kept separate files so they may be streamed to live
  5. Can accumulate more buffers and continue
  
2.
  Build detector, using generic approach to reuse algorithms with profiler.
  Detector will accumulate 'chunks' of waves, each containing:
  - accumulated statistical model, for each sample
  - total number of wavelengths since last period adjustment, to plan drift compensation
  - numer of wavelengths used to accumulate data, to properly scale data

  - [ ] Plan detector approach and separate out shared algorithmic concepts with profiler.
  - [ ] Mark a TODO whenever values are accumulated to verify accuracy is not lost.

3 => updated.  see 'NEW PLAN for detector' far down in file
  1. hold a buffer (dense array) of gaussian distributions for wave + background (signal sum)
     add TODO to consider if the distributions are non-gaussian?
  2. use power/mag convolution to phase-align with profile
  FIX TODO: phase alignment and power measurement could be separate steps.  Then power measurement can integrate
               entire signal to detect under weakest conditions.
  3. signal stats buffer draws a variance and mean plot of wave + background
     since variance and mean sum, can infer variance and mean plot of underlying signal from extrema of buffer
      (a * const + background) - (b * const + background) = (a - b) * const
     use this to identify noise (and optionally value) magnitude of underlying wave, related to a model extreme
      .. may want to produce same buffer in profile
  add TODO to work to resist possible corruption attack by using data from multiple sources
  add TODO to work to resist possible attack by using encrypted timing
make sure to TODO to check
  - the impact of not phase-aligning the wave
  - the significance and accuracy of the measured power of the wave

4.
  What do we do when it doesn't work?
    1. when ROOT downloads, spend 7 minutes building an example, 7 minutes building a charting example, and 7 minutes trying to make a moving charting example; then 7 more minutes time buffer
          if doesn't work after 28 miuntes, try VTK


== monologue cruft ==

PROFILING APPROACH
- FFT will find signal easily.  This can rapidly give approximate period.
      How does FFT narrow period?
      1. a longer buffer gives more accuracy slowly
          needs TIME and MEMORY
      2. including harmonics gives more accuracy rapidly
          needs ALGORITHMS, TIME, and LOUDNESS

- Downsampling: low-memory binary search
    

FFT HARMONIC: any way to differentiate signal from noise easily, to identify real harmonics?  quic


  approach monologue
      if I assumed that loud harmonics were real, then I could just pick the one that gave the best accuracy
      Given this best accuracy range, I could then use a different approach with this narrower range.

SCRATCH THAT!  let's convolve downsampled data

  approach monologue
      If the signal is very loud, I should be able to find it exhaustively by downsampling less and less.
      Downsampling limits the areas I need to search; so say we downsample to freq * 4: we'll have 4 bins,
      4 samples, per period, can identify peak and trough in these four.
      It's LOUD, so we can do this before phase drift happens too much.
      Then decrease the downsampling by 2, and we know have a range to narrow down the metrics over.


    this could even work when signal is weak


    so, sample rate is S, guess frequency is H
    downsample to S / 2 / 2 / 2 ... until S is < H * 8 or somesuch

    how do we determine how much data to accumulate?
    -> if signal is loud enough, only need 1 period.  multiple periods only needed to accomodate variability in phase or frequency.

    STATS: variability in phase and period.
      I'll need to make an assumption about how the signal will behave in the future.
      Part of this can be based on understanding of the signal.  If it is driven by a hardware clock, it will only jitter as much as the clock and hardware details do.
      If it is driven by a multiprocess cpu, it may jitter arbitrarily.
      Let's assume that jitter is gaussian. (alternative we could measure its distribution)


      Shifting =)
      So, one period has a best-guess phase.
      We convolve the nxt period to find its drift.
      On and on for each period.

      We make the assumption that the signal is louder than background noise, implying that each convolution is correct.
      If background noise is gaussian, some will be wrong, but we can investigate signal judgement in future to determine stats metrics.

   Result -> at sample rate D (downsampled from S) we have a map of all periods in buffers.
   Then upsample (total size is n log n ram used, or could recalc) to narrow down, with more convolving.
   Then we have a final map of each period in the signal.

   Do we want to use stats bins for this convolution?  We could.
        Hmmmm what existing stats bins do I have?
        I one that assumes gaussian and only stores metrics; that would be lightweight.
            Later references to underlying data could be added

    If we use gaussian stats bins, then each downsampled bucket is composed of multiple upsampled buckets.
    When we add samples, we can add them to downsampled bins as well as upsampled bins.
    As things become more precise, do we want to adjust the downsampled bins?

  Once the end-result, after N buffers are processed this way?
    We have a mark for the time start for every wave onset.
    We have a sampled distribution of periods given the differences between these time starts.
    This distribution has an average to a given precision, over a given time.
      Best precision of average is precision of 1 value multiplied by period count?

      So 1 value has a precision +- sample time.
      As we add more waveforms, each one divides sample time by count.
      This is best precision.

  Data storage:
    good to get in the habit of storing things in a way that if power is lost data is retained
    so at head of file, keep updating period length, and std deviation, maybe some extra spots for additional metrics
        - average period length
        - max possible precision
        - standard deviation
    file could reference other files that include raw waveform and marks for wave onset guesses

    std dev could lie!
      each individual period has its own accuracy.

    So we have a set of onsets that are only +- the sample time.
    We can infer the average of the underlying distribution, but its hard to easily infer details if they are smaller than the sample time.
    Std deviation sounds most interesting if it is larger than the sample time.  Under that, we're just seeing the sampling error.

    What we'll need to infer when to give up in the detecting half will a concept of the _minimum_ precision of the period.
    This will let us infer when we cannot detect the data.

    Two options:
      1. period deviates more than sample time.  given distribution of this deviation, is possible signa is gone.  is also possible it is still around; more likely
      2. period deviates less than sample time.  Not enough information to determine how much period may drift outside of profile time.

        (although with multiple recordings we could measure drift, this would be the same as the deviation exceeding the sample time.  it's just a longer recording)
      Given option #2, it seems we'd assume the maximum precision is the minimum precision.  The signal is gone when it will no longer constructively interfere with itself.
    

DETECTION APPROACH

  TODO: update metafile once signal is found.  will require metafile to reference multiple logfiles.
  TODO: store model of signal details that updates as more data is found

  - keep a wave buffer of correct size
  - use period to categorize samples into wave buffer
  
  wave buffer could be raw averages, or more detailed metrics such as std dev, or even a matrix of histograms

      Now, histograms are reasonable because the samples have a small resolution.
      However, histograms don't account for changing in recording conditions

    Profile data would ideally REMOVE background noise !  how the heck would we do that?

          I think it is doable by deconvolving the distributions.
          We'd need a distribution of background noise.
          We'd have to extract it from the signal

    it is notable that if your signal is very loud you can record it and just ignore the background noies x >> noise
        consider a little more strongly holding details of the variation in the signal.
        
    this variation is recorded in the reference to the logfile and the onset times marked in the metafile, as precisely as is possible
    producing a summary of it is useful for detection, but the detector could do it
        it fits in to the conceptual domain of the profiler
      that's correct

    maybe allow some space in the metafile for extra data, somehow
    
    but what right now to use for the detector?
    so, with a reference to the profile data, we have some signal data ...
  
    we can model the signal and compare to the recording and update the profile file.  this will be useful later.

    but for now we just need to know the precise period and determine the magnitude of our signal given that period
    once we identify we have the signal, we can actually update the metafile with more precision (TODO for later)

    okay, so we'll have to form some model of the signal.
    we have signal + background noise
    I think I figured this out already, somewhat.  Maybe a plan is to look through my existing code and try to reuse whatever my approach was.
    It involved combining distributions that were either histograms or gaussian; using stats properties i googled but don't quite understand

        if we have an arbitrary signal, we can look at the extrema: highest point and lowest point
        how will these extrema move, in the presence of background noise, as the magnitude of the signal changes?

        if we are phase-aligned, then we can use signed data, and the average of the data will represent the underlying value; since the background signal is noise
        we could then assume that our recording is proportional to the model and the profile

      once we are phase-aligned, then we know our period very precisely.

        so, for a real signal, that's relevent: here my signal is noise itself.
        phase-aligning may not help here.

      the signal mostly does not repeat, just the mean of its amplitude
        likely the signal will have some phase-aligned portions as power is provided and cut to toggle it

      so I have a big set of recordings, and my informationo n them has to do with their distributions, so distribution math makes sense

      when the noise is active, we have B + F (background + foreground)
      when the noise is inactive, we have only B
      we have a ton of samples, and we can convolve the mean of the magnitude or power with the model to find the phase

      the question is what is the intensity of the signal?  how much has it been attenuated?
      if the background were weak enough, we could juts take the magnitude of the signal as its magnitude
      but once it gets smaller than the background noise, we'll need to use stats

      So first we phase-align it
      then we collect a distribution of signal noise, and a distribution of background noise

      there's an idea here that i like of collecting such distributions also during the profiling phase, which could make the code more general and yield a deeper understanding of the signal

      but for noiscillate, we could assume that the peaks and valleys are flat -- IF the underlying signal actually behaves that way.  it's likely at least to be a good approximation
        there could be a sudden burst at start or stop that would make that slightly inaccurate, but it should be fine

        let's try pursuing this a little
        a signal will have consistent components and random components
          we could model each point in waveform as a distribution with a mean
          if the signal is attenuated, the mean will shrink, and the variance of the distribution will shrink, I think, something like that

          so given that model of each sample in the wave having a distribution, we could compare it

        the problem is that these are complex-valued signals recorded with an SDR.
        so if there was an underlying signal, it would need to be phase-aligned to show up on such a distribution

          a signal is made of a sum of fourier components
          each of these components has a separate phase and will drift by a known/different amount
          you can break these into pieces and track them all
          because they drift within known amonts

        okay now I'm thinking a 3-dimensional model of a signal ... yes.
        
        nice =D
        okay each point in time has a spectrum, and each spectrum has a distribution
        
        to do this I think I would need to 1st determine the precise period based on the changes in power
        given the precision of that period, I know the phase drift
        
            phase also relates to underlying accuracy of period, unforunately and fortunately
              that sounds complex, TODO: think about that
          if we assume the period is more accurate than the length in time of an individual piece of the wave (which is actually reasonable), then:
            we can calculate the samping error for each recording of the wave
            and adjust to line up the periods.
            we can then separate each sample into a known average, which is a complex number, and background noise.
            we'll have to try this for the spectrum of frequencies the device might emit.

            TODO: identify sampling error for jittering signals, noting that it would be easy to jitter a signal to make this technique not work until this is done
            NOTE: if a device were _hidden_ and emitted a _repeating_ signal from some component, randomizing delays in the signal would make the details cancel themselves out.  it would make it much harder to find it
                  from anything other than its power.

Profile #2:
    This profile approach attempts to begin to separate underlying signals from noise.
    What's available in the result gives a clue into what the underlying period jitter is; components that are a higher frequency than the jitter will fade away following a precise curve.
    
    Can I quickly add this, or is it silly?
        It's reasonable because the device may not actually give square wave noise.  But not much should be needed.  A plot of just thep ower without phase information would be sufficient.
        WHEW.

Okay, detecting power.
    The model wave can be considered a sequence of distributions.  Background noise could be ignored since it will be small, but note that we'll have background noise in the detection phase, so it's reasonable to consider.

    Peak and trough of model can all be considered B + F(x) wrt power.
    both B and F are distributions.
    All we have are the B+F(x) = T(x) distributions; but each T(x) contains B(x)
    So the solution here has to do with how the underlying distributions sum.
    If they're gaussian, it's not that hard.
    If they're not ... convolution lets us add and subtract them, but that's the only technique I have offhand.  Some quick way to do it?
          I could add them all and get n*B + sum(F) ...  B is now much stronger than F.  The average of the resulting distribution is likely to be much closer to the average of B than the average of F.
        


        what does this mean ... T(x) is a distribution of +- real values.
            B(x) is a distribution of +- real values
            F(x) is a distribution of +- real values
            
          when I add a distribution, what does it mean?
          it means the distribution of the underlying data summed
          
          so I can easily get the distribution of the whole signal ... then we have B + sum(F)/n, I think ...
        sounds a litle unsure, let's assume gaussian and add a TODO

    assume gaussian, add a TODO to use specific distributions

      okay, so I have B + F(x) where B and F are defined by their mean and variance
        the meansn sum, the variances sum
        the mean and variance I have are
          m(T(x)) = m(B) + m(F(x))
          v(T(x)) = v(B) + v(F(x))

          I'd like m(F(x)) to calculate power
          for that, I'll need m(B)
        
          hmm what about ratio?


          m(T(high)) = m(B) + m(F(high))
          m(T(low)) = m(B) + m(F(low))

          m(T(high)) - m(T(low)) = m(F(high)) - m(F(low))

          The arithmetic difference between the means of the different spots of the signals represents the background mean (within proper stats stuff)
          So I can compare all spots in the signal to the most extreme value.  I think ...

              this should break down ? when it's too small ?
              maybe not.  m(B) may be a bajillion, but we'll still get small changes in T(x)
                it doesn't intuitively feel ilke these small changes are going to be arithmetically equal to changes in the mean.
              
          what are we measuring here, at each spot in the signal?
            we actually have a distribution of samples from that spot.  precise samples, dsitribtuion

          so changes in the mean are changes in the mean of that distribution
          hmm if I ahve two guassian distributions and sum them, the means sum.  if they're very large and the sample is small, etc, inaccurate
          
    so some stats will be needed here


MONOLOGUE:
  So ... when profiling i considered convolving more than just 1 period, maybe 3 surroudning
that idea could be generalized to convolve n surrounding.
If this were done, it would work on weak as well as strong signals, I believe.
    this is the approach tempest-sdr uses, I bet.

The convolution function is interesting in that it has no concept of periodicity.
    It compares each spot to each compared spot, and just sums the result.
    What we want to do is different: we want to combine n spots from one, and n spots from the other, and
    compare them all.  This is doable with matrix lib.  I think I wrote some code to do it already.
    
If I were to do this, I would get more data and accuracy for each comparison from the profiler.
I'd additionally get the onset times more accurately, but there would be fewer of them: the onsets would be
the averages of the neighboring onsets.
The result would be more resilient in the face of noise and spurious things.
So, that's notable.
If I'm doing a recording next to an emitter, and there is a sudden problem, like a spark, one of my periods
will be totally wrong.  But taking the 3 adjacent ones doesn't really fix that; it just makes the period be
the average of the surrounding ones.

Then, if I am distant from the emitter and want to find it when it has been lost, doing this would be more
useful.  The period has changed slihtly, and the profiling technique will not longer work because the signal
is too weak.  But if I accumulate many periods in this way, then I should be able to use this profiling technique.

The simple approach of adding to each surroudning will slow down the processing by a factor of n.  Each period
onset will become the averaeg of n surrounding onsets.
Alternatively I can accumulate n periods until I find their average.  wasn't I doing this before ???? this was like
my first approach.  what went wrong?
The problem was that I was iterating each possible period sample-by-sample.  Now I'm starting downsampled, it's
more reasonable to do.

Okay, consider briefly that we have need to record 100 periods to see the signal at all (or a million, or a billion)
call that number C.
The ability to find the period via downsampling is highly limited based on how C relates to our accuracy of guessing
the period.  Notably, downsampling doesn't even make use of a period guess in its present implementation.
The phase will drift, creating destructive interference, resulting in a nonexistent result.
But there's a "sweet range" in there, where combining periods allows the SNR to increase, before the phase drift
causes it to cancel out.

If we're recovering from a loss of the signal, that "sweet range" could be very useful to recover the signal,
if we can fix the downsampling problem where there is phase incorrectness even if we know the phase very well.
If the period is exhibiting brownian motion, we can guess how far it may have drifted to, but there may have been
too many periods gone through to try all the guesses exhaustively.

The problem is that the current downsampling implementation can only 'hug near' guesses that are powers of 2.
Could I fix that, I wonder?
I'd want to do the downsampling differently.  It mgiht be slightly slower.
I'd want to sum over the actual periods, given the guess period.

I think I've already written code to do this more complicated downsampling.  I don't know if it will be reasonably
fast; if it adds linear complexity I'd judge it too slow.

What needs to be done?
Right now:
  log(n) times for each downsampling:
    constant: form a view of the buffer with outer stride = 2
    n but inside eigen:      sum each row

Same approach may not work because we can't rely on the 2-poer property, to retain phase in downsampled buffer
Behavior of phase retention:
  sum each predicted period
    so the most downsampled copy would contain sums of entire periods
    the least downsampled copy would sum pairs of data
    the 2nd-most downsampled copy would sum halves of periods
one approach would be to start with most-downsampled, but keep references to the original data
do it period-by-period
so, for each period (n/log(n) kinda):
    log(n) times for each downsamping:
      sum the data that will go in the downsample
this is similarly n log(n) sums, but the difference is that the loops across the rows are outside eigen rather
than inside
if I could invert it to use a map, the behavior would be the same between them ...
oh no, here's the issue:
    if it's not a power of 2, then I think I have to redo the sums from the original data each time
    which turns it into n sums instead of log(n) sums if we reuse the previous ones
    we have to reuse the previous data in the shrinking loops to gain the speed advantage
    
to reuse the data I have to pay attention to how the periods are shifting
the breaks between periods need to be kept distinct
0 1 2 3 4 5 6 7 8 9
period = 2.5
|    |    |    |    |
0   2   4   6   8
the next downsample result might contain 0, 4, 6, 8, dropping 2
or it could contain [0+1+2],[3+4],[5+6+7],[8+9], which would be more accurate but downsampled more
hmm

0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9
period = 9.5       |                  |                  |                  |
0   2   4   6   8   0   2   4   6   8   0   2   4   6   8   0   2   4   6   8

the ideal fully downsampled result would be:
0                   0                 9                   9
this is impossible to get from the 2nd downsample because 9 has been absorbed.
some data would have to be taken from the original to do this.

alternatievly we could start with that final result, and add in details from above?
so, we have each period onset, which we could turn into rows with a map
we want the downsampled pieces from that map
the row length is not a power of two
if we could break the columns in two and sum them, we could produce really useful downsampled data.
say the period is a prime number.
to find the first guess, we want to be able to divide it into chunks
we would need to divide it into chunks that are not equal.
that's probably the solution, divide into inequal chunks.

so i'd want to plan how to do this division of the columns into inequal chunks, and build each chunk out
of sums of its inequal pieces

this is not conducive to a map, because the rows are of different lengths in the interim bits.
how do I find the phase in the first step if the period is a prime number?
    by phase I mean the true period, guess period is a prime number, last known period

for one thing, if you know the accuracy of your guess you dont' need to downsample so far, could perhaps pick
    nonprime situation
if the guess period is nonprime, then I can downsample easily via one of the prime factors, kind of funny unideal

it's hard to tell if this is an easy problem and my mind is having issues, or if this is an unreasonable problem
that i should forget about

  

FAILURE PLAN:
  - likely I have made an error somewhere and will need to adjust this plan as I work on it
    maybe I'll want to output the results of each step and verify they are what I expect
    I could try to avoid spending a lot fo time working on something without checking that it works.

        It's _really_ nice to be able to graph what is being recorded, in real time.  This is very helpful for diagnosing things.
        Instead, I've been printing numeric metrics to the screen, which uses much less of my senses to verify correctness, make sure it is what I expect.

    Maybe we can link in a small graph.  Take the work from emap?
          emap's graph is part of a big package .. it doesn't leave room for labeling things etc

      Maybe take the graphics lib from emap and make a homebrew graphing class, just to watch things.
      Or link in with gnuradio, but they are also a big package, do they allow labeling? no, it's just a more generic approach where my work is more reusable in case I go down a dead end
      
      there's got to be a realtime charting lib that can be quickly pulled in.  7 minutes

      ROOT is a c++ library for scientific analysis of data.  This would really help me have algorithms more accessible, and it has plotting/charting.  Not sure how easy it is.
        Problem is learning curve.  I could get distracted while learning it.  Gratification of accomplishment is distant.
      not sure how heavyweight root is, might be hard to use embedded.  it tends to store function definitions at runtime, which is unneccessary and uses lots of ram, and possible cpu parsing them
      VTK has just the charting without all the function definitions
      ROOT is used by CERN and likely actively maintained

    ROOT is downloading; 2 hrs. maybe review when download completes.



== more detection ==
My detection plan at the top is not fully clear.

Buffer of distributions for wave + background: since it is not phae-aligned yet, I'm assuming that I mean
 to keep a separate distribution for each piece of the wave.  "buffer of distributions" -- that must be where
 "buffer" comes into play; an array of distributions.

fixed above

2 appears clear: convolve with model
3 looks like I am assuming that the peak represents absolute magnitude.
Won't this require using the raw data, rather than the absolute data?
Need to do something fancier here, since my signal is noise and summing the raw data will yield 0.

I think I was rushed, so just considering poewr peak.  But I'm thinking if signal is weaker than ambient noise,
this will just be the mabient noise magnitude and not the signal magnitude.

If we have a model with noise removed, we could assume that the signal we have is model * constant + ambience.
I'll have to use stats math to extract model from the power.

that's a big FIX TODO.

I have a strong memory that I found that the difference between values of the signal is significant.
Have to monologue that up.  Make the profile for now.

The question here is just how the mean of the std dev sums.  I've solved it before.

--
in detector, may want to use stats to determine likelihood wave is there ... perhaps even crucial to do this !
written code for this already ...
the wave we plot is of sampling distributions from the source
we want the likelihood that the most extreme sampling distribution is non-random
this gives two metrics: noise detected, signal detected

we'll want to determine the 'noise wave' and 'signal wave' from the model profiled wave
this will require a loud wave
the current profile determines a model wave, but it uses magnitude
my plan here used the standard deviation of the raw value, not the magnitude of the raw value
the profiler could be altered to also do this ...
or i could write another class to produce the data again, using the known period ...

the profiler forms a model wave by summing the underlying wave, using magnitudes

the detector keeps a model in the form of an array of stats information or distributions
i could make a noise/value signal model class
and convert the profiler to fill it
then read from it in the detector
the profile would have to convolve differently
right now it can use 'vector' operations
but i haven't written stats stuff for vectors of distributions
also, comparing the values may change; it could be reasonable to store both
of course the sums of the magnitudes that the profile uses is very similar to the standard deviations or variances
used by the new model type

okay we can do this right
a wave of noise is a sampling distribution: each sample of the wave, is a sample of the noise distribution, scaled
by the noise envelope of the wave, basically
so the model wave is a wave of variances / standard deviations
this would mean making the profiler more complex to understand the difference between variance and mean.
it seems simplest to assume that abs(val) = std dev, here, because it's mostly noise, likely all
(if mean were nonzero, i guess you'd want to compare it to std dev converted to 50%, if it's smaller use
 std dev, if it's larger use value)

okay 2nd class approach

so, detector might have 2 or 3 phases
- develop wave model (could happen in synchrony of profile) (can continue updating)
  -> stats model is passed on: vector of variances and means
- phase-align (can continue updating)
- calculate power (can be done as soon as phase-aligned)

okay, model needs updating, filling with periods
when power _changes_ power can be compared to model from elsewhere
new model is updated, and comparison with old determines power

is old needed to determine power?
for noise, we take variance of each point in wave, and compare with a constant point in wave.
this difference gives area under curve
old might only be needed for phase-alignment, as power measurement is different depending on phaes
robustness could be increased by simply picking minimum or maximum to compare with ...

 /-\
-   -_

I'm calculating integral(x - x(r),t)
i include the subtractino because it removes the background noise component, which is present in both samples
but mathematically, this should be equivalent to
  integral(x,t) - x(r)
      okay, mathematically, that's wrong: you need to multiply by t

integral(x,y) - x(r) * t

  the accuracy of this result kind of depends on the accuracy of one sample of the wave, unfortunately.
  i'd kind of like to use some stats here.  first let me see if i can note this simpler approach

    i think it's somewhat reasonable to not worry abotu phase alignment, and compare with the lowest or highest
    sampling distribution in the wave

okay, so I could use a class that just reads a mess of waves, and spews out their power as they come in!
  it would fill a model class.
  there are so many waves, it's okay to toss out the profile data for now.  it violates our principal of logging;
  it's important to log to detect bugs, and this code will only be most useful for development and not as useful
  for ongoing maintainence, etc ... can still implement writing to file and there is too much data in the air to
  store any significant chunk of it in a raw format like this
  could save the model.  lets me see the device as its dying.
: okay, class to build stats profile of raw wave
  then that class can spew out power etc

oh my goodness
can i combine detector with profiler?
as detector runs it will need to at least update phase, and when it updates phase it gains information on
period more precisely
let's increase precision of double output to see if we store constantly-increasing information on period
length

hrm the result exhausts the precision of a double pretty fast, and then doesn't move.
this means the deviation of the signal from a simple rational number is smaller than the precision of a double.
the sample clock inside the rtl-sdr is not synced with the time of day, or the clock in the emitter
so eventually it will drift

okay, with a fix, the wavelength has not exhausted the precision of a double.
naturally, since the STD is larger than that, and each one updates it

i've determined that different runs actually DO yield different wavelengths
could be due to drift in the emitter, but more likely I'm thinking it's due to changes in the behavior of the
clock in the receiver

this may be related to the ppm of the receiver, and can be correct for without finding the signal and adjusting
for it.  or, it may be that the signal drifts enough that it will need to be found and adjusted.
ppm of the receiver: this likely drifts (should test), and will need to be adjusted for
signal drift: this will exist regardless ....
if the ppm change were considered signal drift, we could produce the result sooner
  the implication of such a change would be initial signal drift.
    stronger weak signals would be undetectable
    once signal is found, how would be find real period?
    have to detect signal _again_, and determine phase change.

---
detector updating period.

detector reads waves until it detects signal, then stores as chunk
detector then reads more waves until it detects signal again, then stores as chunk
two chunks can be combined into one chunk with adjusted properties by examining differences in detected signal
two possibilities:
  1. one or both detections were spurious.
        -> this will usually, but not always, show up in the phase adjustment being too large
  2. detections were not spurious.  improved wavelength is found.
can then double detection time and run again, refining wavelength more.
  once so many are built up that detecting takes forever, what then?
      now we are reading samples but have no new information on the wave
      we still want to accumulate data until we have power information, and report power
      good time to retune

on retune, what then?
  -> we know wavelength, in theory
     properties of radio imply retuning keeps us in-phase
      but shape of wave may change ... should use a new model

- [X] can this replace profiler? seems like it does!
- [X] how to handle new wave model?
    i think it still works the same.  we assume period is the same, so we can keep updating
        phase adjustment is more confusing .... how do we compare phase if wave shape may change?
        we'll need a model at that frequency
        so i guess phase alignment restarts.  need to build first buffer until wave is detected
        then acn compare with that chunk to adjust
    so need to split chunks into an additional part
    - number of wavelengths period is known by
    - number of wavelengths used to accumulate data, to properly scale data
- [X] consider accumulation error
    we are using similar accumulation to create wave model to compare data against
    this accumulation, also used inside stats dist, ended up producing significant error
      i was expecting error due to summing integers cast as doubles to a size large enough that some precision
      was lost in the sum compared to the precision in the numbers summed
      but this error seems large compared to the error i would expect from that

DETECTOR
  we start at one frequency and build up a wave.  once we verify that wave exists, we can store information on it.
  This information accumulates, slowly, into a single structure that models the wave.
  When retuning, we'll get a wave model at a different frequency.
  We can combine the period information with the old frequency, but not the model information.
  Perhaps keep a model at every frequency: that's a lot of models, maybe condense the storage representation,
   or optionally just store the power.  Keeping all those models could give a lot fo information on the underlying
   signal: also allows for phase adjustment when retuning to that frequency again.
  
  So we accumulate the period, and then separately the models.
  A verification is that the phase adjustment on tuning back to an old frequency, lines up with the phase of the
  old model.
  Information for period:
    number of wavelengths
    number of samples
  Information for model:
    stats for each sample
    number of wavelengths
  
  Accumulated vs. active?
  Active, we'll have a model with sample stats.
  We'll need to compare the active model with a stored model for that frequency.
  Once we have both, we can convolve to determine phase shift, and adjust the stored info?
    -> if the phase shift is large, the model will be really blurry and probably shouldn't be combined with future data.
       any raw data still stored can be reinterpreted with the new wavelength.
       this could be a TODO, but more accuracy if old model dropped
    -> if the phase shift is just one sample, we can just rotate the new model and merge it with the old model
  Note: this algorithm would benefit from being able to rewind data
    
  so: === NEW PLAN for detector ==
  - total wavelengths
  - total samples
  - container for stats models, by frequency:
      1. "past" object for detected waves, summed together
      2. "active' object for undetected waves
      each has:
      - stats for each model sample
      - reference for wave merging and shifting, perhaps:
        - sample time start or end
        - number of wavelengths involved
        - number of samples involved


  behavior:
    - receives a chunk of samples, and current tuning frequency
    - accumulate into active model
    - detect if the data in the model has reached a threshold of significance. if so:
      - if past model present:
        - convolve with past model
        - shift to phase-align
        - store updated wavelength measure
        [ add TODO to possibly adjust interim data ]
        [ add TODO to possibly handle more accurate onset time guesses, interpolated from shift ]
      - accumulate active model into past model
      - update wavelength measures

-> separate model from detector.
   model for profiler just summed
   model for detector may use stats accumulators for now, many of them

next: generalize with profiler
similarities between profiler and detector:
  - read a bunch of samples, then after processing them update information
      information may include a change in the cutoff between last block of data and next block of data
      but because detector downsamples can't really pull that out
  - keeps a model of wave that is accumulated into (detector upgrades this to handle multiple freqs)
  - picks areas of data, decides they are periods, and accumulates them into model
    -> profiler produces new data from data, and does this to the new data before doing it to the original data
  - produces updated average of wavelength


how can I add generality to the new detector class, to allow the profiler to share code with it more?
  they both pick areas from a datastream that are wavelengths
  - I could add another layer prior to the area picking, that can produce more datastreams and call out
    -> kind of like a gnuradio block with many outputs
  or I could let the layers of data in the profiler be 'special' and just use them as a way to pick onsets in
    the original data.  this would increase code in the profiler to handle the special case of the original data.
  the profiler produces each onset sequentially.  the detector produces them all at once.

  -> idea here of picking onset indices in an array of data, and merging them into a model.
  sounds like a good piece of shared behavior to pull out

-- older:
how to handle large measure with small active wave?
I think we can actually combine the information

hmm to find phase drift i will need to store current model separate from prior model, in order to combine them
might need two vectors of models !
so:
    when i track a wave i wan to track the phase drift
    i'll need something to compare current wave with, see if phase has drifted
    if i'm just on one frequency, i can wait until i see wave, store it in model #1
          then wait until I see wave again, storing it in model #2, and once i see it merge #2 down to #1, update
          information
    when i tune, things get cnofusing.
        if I haev wave #1 in old tuning, I can likely use it much later somehow, by adjusting for drift (TODO)
        if I have wave #2 in old tuning, I _could_ store it ... then I'd have to adjust for drift wrt it as well
              I could halve my memory usage by not storing it
              I could also request a tuning timing such that tuning only happens at helpful times
              I could also take wave #2 and merge it into wave #1 .... what a decision
        i'll store both waves
    
          
--
okay, fuzzy bit in detector plan:
note: _nextOnset is not updated

how will I handle frequency hopping?
when we leave a frequency we may have a past model stored there
and we may have an active model stored there
    the active model will have been read a certain way

it looks like _nextOnset is useful mostly for the next upcoming data, and doesn't need to be stored across freqs
but when we tune to an old active wave, we'll want to adjust nextOnset !
  we can probably adjust nextOnset when the wavelength measure is adjusted

and since all freqs have the same period, nextOnset isn't needed per-freq

--
here's another issue ...
when we are on a new frequency, we have wavelength information stored from the old frequency to inform us.
But we don't have an old wave shape in order to update the latest chunk with a phase adjustment.
We can't use this latest chunk of data to inform the wavelength: we need two chunks.

how to handle these two chunks? and total counts?
well we have a period count and a sample count from prev freqs

when we store first wave, not much info.
when we store second wave, we can adjust those two.
we now can see if there is phase drift between them, and get an approximation for period
but since we don't know where the wave onset is, we can't line these periods up with the data gathered
  from the other frequencies.

we could make the assumption that the tuned-to freq is mostly in-phase.  this would allow to continue to
 accumulate data.  would want to wait until our phase settled before tuning, I suppose

given that the first set of buffers from a new tuning have an unknown phase drift, is there a way to
  include this lack of information in a later result?

for one thing, when we retune back there, we can continue to adjust
when we return back to the original frequency, we'll

here we go:
  when we retune back to the original frequency, we can determine our first phase offset.
  this phase offset can be used to rotate recordings from other frequencies to line up.
  I'll want to store with models the time at which their rotation best approximates the phase.
